% Latex template: https://github.com/mqTeXUsers/Macquarie-University-Beamer-Theme

% Slide Masters:

% Title
% Text
% 2 column
% Full-image
% Bibliography
% Closing
 
\documentclass[aspectratio=169, 11pt]{beamer} % Aspect ratio
% https://tex.stackexchange.com/a/14339/5483 
% Possible values: 1610, 169, 149, 54, 43 and 32.
% 169 = 16:9

\PassOptionsToPackage{table}{xcolor}    %https://tex.stackexchange.com/a/5365/5483

%https://www.overleaf.com/read/qnhgkdqyhzjv
\PassOptionsToPackage{contents={}}{background}


\usetheme{macquarie}
\usepackage{multicol} % https://tex.stackexchange.com/a/396018/5483
\usepackage{xurl}
\usepackage[british]{babel}       % Set language
% \usepackage[utf8x]{inputenc}      % Set encoding
\usepackage{colortbl}
\mode<presentation>           % Set options
{
  \usetheme{default}          % Set theme
  \usecolortheme{default}         % Set colors
  \usefonttheme{default}          % Set font theme
  \setbeamertemplate{caption}[numbered] % Set caption to be numbered
}

% Uncomment this to have the outline at the beginning of each section highlighted.
%\AtBeginSection[]
%{
%  \begin{frame}{Outline}
%    \tableofcontents[currentsection]
%  \end{frame}
%}

\usepackage{graphicx}         % For including figures
\usepackage{booktabs}         % For table rules
\usepackage{hyperref}         % For cross-referencing


\usepackage{pdfpages}

\usepackage{textcomp}

%\usepackage{enumitem} % https://tex.stackexchange.com/a/2292/5483
\usepackage[shortlabels]{enumitem}

%https://tex.stackexchange.com/a/371844/5483
\setbeamerfont{bibliography entry author}{size=\tiny}
\setbeamerfont{bibliography entry title}{size=\tiny}
\setbeamerfont{bibliography entry location}{size=\tiny}
\setbeamerfont{bibliography entry note}{size=\tiny}
\setbeamerfont{bibliography item}{size=\tiny}

%https://tex.stackexchange.com/q/333587/5483
%TODO SHAWN REPLACE OSF URL
%\setbeamertemplate{footline}{\strut~\texttt{https://github.com/MQ-FOAR705/MQ-FOAR705-Week1}\hfill\insertframenumber~/~\inserttotalframenumber\strut~~~}

\newcommand{\makeslide}[4]{%
\section{\huge#1\\\vspace{12pt}\Large#2}
\setbeamercolor{background canvas}{bg=}
\includepdf[pages=1-2,
offset=0 -1mm,
%delta=0 1mm
%fitpaper
scale=#4,
%tempatesize={12.8}{8}
%,templatesize= {\beamer@paperwidth}{\beamer@paperheight}
]{presentations/#3}

}

%Lessons learned from Data Analysis projects in Natural Language Processing with Japanese and Security Studies data - Shell Scripts, Jupyter Notebooks, and the value of doctests
\title{Digital Humanities 2 Minute Madness} % Presentation title
\author{Semester 2 2019 FOAR705 Students}               % Presentation author
\institute{Faculty of Arts}         % Author affiliation
\date{Tuesday 14 November 2019}                 % Today's date  
\begin{document}

% Title page
% This page includes the informations defined earlier including title, author/s, affiliation/s and the date
% \begin{frame}[noframenumbering]

\maketitle


\begin{frame}{Today's Plan}

These presentations will run in strict two minute intervals. The objective is to interest audience members enough to ask questions during our ``breaks." 

This idea, inspired by the European Geosciences Union's PiCO Presentations\texttrademark~aims to: ``PICO combines the advantages of both oral and poster presentations... Every PICO author first presents his/her work orally. Afterwards, all session attendees have enough time... to hold discussions with the author and with their colleagues..." (https://egu2018.eu/pico.html)

\end{frame}
\begin{frame}{Today's Plan}

Three rough topical groups: 

\begin{itemize}[label=\textbullet]
    \item Texts. Machines. Data.
    \item Organisation, Presentation, and Version Control
    \item Application Programming Interfacess and Ex Miscellanea
\end{itemize}
\end{frame}

% \end{frame}

% \begin{frame}{Table of Contents}
%   \tableofcontents
% \end{frame}

\section{Texts. Machines. Data.}

% Sophie Avard
% Jan Jagueta
% Aaron Hammond
% John Hundley
% - Gabriel Wallis
% Isaac Harrison
% Damoon Jehani
% Ellen Kirkpatrick
% Osmond Chiu
% Bree Kelly

\makeslide{Improving the Efficiency of Textual Analysis in R}{Sophie Avard}{sophie_a.pdf}{0.98}

\makeslide{Corpus Word Frequency Analysis}{Jan Jugueta}{jan.pdf}{0.98}

\makeslide{Conversion and Analysis From Multiple Data Sources}{Aaron Hammond}{aaron.pdf}{0.98}

\makeslide{Voyant and the Data Life Cycle}{John Hundley}{john.pdf}{1}

\makeslide{Evernote, Zotero, and File Juggler}{Damoon Jehani}{damoon.pdf}{0.98}

\makeslide{Connecting and Compiling Sources}{Isaac Harrison}{isaac.pdf}{0.98}

\makeslide{Identifying key themes and efficient storage for qualitative
analysis}{Ellen Kirkpatrick}{ellen.pdf}{0.98}

\makeslide{Improving the Organisation of Sources and References}{Osmond Chiu}{osmond.pdf}{0.98}

\makeslide{The Hieroglyphics Initiative: Sharing Information Made Easy}{Bree Kelly}{bree.pdf}{0.98}

\begin{frame}{Discussion}

Spread around the room. For the next 15-30 minutes, wander up to individual presenters and ask them questions.

\end{frame}


\section{Organisation, Presentation, and Version Control}

% Emily Hunt
% - Fire absence: Andrew White
% Jeremy Amin
% Dylan Wheeler
% Tom Kongonis
% Isaac Harrison
% - Mona Ghai
% - Tobias Hansson
% Sheriden Goldie
% Lauren Franks


\makeslide{Improving the Writing and Organising of Notes on a Film}{Emily Hunt}{emily.pdf}{0.98}

\makeslide{Robust feedback tracking for Docx et al}{Jeremy Amin}{jeremy1.pdf}{0.98}


\makeslide{Improving Organisation of Music Production Projects}{Dylan Wheeler}{dylan.pdf}{0.98}

\makeslide{Using the Shell Command to View Multiple Text Translations}{Thomas Kongonis}{thomas.pdf}{0.98}

\makeslide{Planning for Publication}{Sheriden Goldie}{sheri.pdf}{0.98}

\makeslide{Publicly Presenting Interesting Research}{Lauren Franks}{lauren.pdf}{0.98}


\begin{frame}{Discussion}

Spread around the room. For the next 15-30 minutes, wander up to individual presenters and ask them questions.

\end{frame}

\section{Application Programming Interfacess and Ex Miscellanea}


% Tom Duloy
% Matthew Clark
% Roslyn Walker
% - Bart Wojcik
% Jesse Grubesich
% - Sick, Kylie Reynolds
% Jack Mathieson
% Sophie Wallace
% Georgia Rutherford


\makeslide{Using FAIMS Mobile to Collect Diverse Data Through
Collaborative Interviews}{Thomas Duloy}{matthew.pdf}{0.98}


\makeslide{Discogs: Producing a Techno Data Set for Machine Learning}{Matthew Clark}{matthew.pdf}{0.98}

\makeslide{Discourse Network Twitter for Rtweet}{Roslyn Walker}{roslyn.pdf}{0.98}

\makeslide{Croatian and Serbian to English Translation}{Jesse Grubesich}{jesse.pdf}{0.98}

\makeslide{Extracting Interview Recordings}{Jack Mathieson}{jack.pdf}{0.98}

\makeslide{Managing Qualitative Data In The Field:
Automation, Images and Metadata}{Sophie Wallace}{georgia.pdf}{0.98}

\makeslide{Hyptothes.is: Local Backups and Analysis}{Georgia Rutherford}{georgia.pdf}{0.98}

\begin{frame}{Discussion}

Spread around the room. For the next 15-30 minutes, wander up to individual presenters and ask them questions.

\end{frame}


% \makeslide{Two common and time consuming tasks when doing research:}{Osmond Chiu}{osmund.pdf}
% \begin{frame}{Japanese Advertising Research}
% \begin{columns}
% \column{0.5\textwidth}

% \begin{itemize}[label=\textbullet]
%     \item Corpus donated by a Japanese advertising agency. Very simple CSV in Japanese.
%     \item ``47 insurance companies with a total segmented and filtered token count, after removing stopwords of 29,486 [tokens].'' 
%     \item Parsed via MeCAB (Using the mecab-ipadic-neologd dictionary, trained on web content.)

% \end{itemize}




% \column{0.5\textwidth}

% \begin{figure}
%     % \centering
%     \includegraphics[width=\textwidth]{figures/Figure1.png}
%     \caption{From `Securing Life in Trust: Corpus-based Keyword Analysis of Japanese Insurance TV Commercials' by Svetanant and Ballsun-Stanton. Do not distribute.}
%     \label{fig:userstory}
% \end{figure}

% % \end{column}
% \end{columns}
% \end{frame}
% \begin{frame}{Social Media Research}
% \begin{columns}
% \column{0.5\textwidth}
% \begin{itemize}[label=\textbullet]
%     \item Studying keyword use of local groups.
%     \item Data set from mastodon currently downloading. At 1GB of JSON already. 
%     \item Using Twitter Historical Power Track to run a single massive query for \$2,000
%     \item My first official ``big-data'' (It doesn't run on my laptop)
%     \item Interacting with the raw APIs via python's `requests' module.

% \end{itemize}
% \column{0.5\textwidth}
% \begin{figure}
%     % \centering
%     \includegraphics[height=.7\textheight]{figures/blur.png}
%     \caption{Lexical Dispersion plot by Brian Ballsun-Stanton. Blurred due to sensitive ongoing research.}
%     \label{fig:blur}
% \end{figure}
% \end{columns}

% \end{frame}
% \section{Natural Language Processing}

% \begin{frame}{Tools which have produced results}

% \begin{itemize}[label=\textbullet]
% \item Python3
% \item The NLTK book.
% \item MeCAB for Japanese
% \item Log-likelihood computation by Rayson and Garside 2000
% \item Checking dates on stack overflow posts
% \item Filesender
% \item Making sure my code is resume-able
% \item Git
% \item rsub/rmate
% \item byobu
% \end{itemize}
% \end{frame}
% \begin{frame}{Python Modules}

% \begin{itemize}[label=\textbullet]
% \item Using spaCy for tokenisation, lemmatisation. NLTK for stemming.
% \item Matplotlib for a Lexical Dispersion Plot
% \item tqdm
% \item dateparser and Delorean
% \item tldextract
% \item matplotlib 
% \item sqlitedict
% \item wordcloud
% \end{itemize}
% \end{frame}
% \section{Doctesting}
% \begin{frame}{When do I doctest?}
% \begin{columns}
% \column{0.5\textwidth}
% \begin{itemize}
% \item Very slim form of unit testing: 
% \item ``Making sure each function I write does what I want it to do and returns output I want it to return.'' 
% \item Great tool for catching errors introduced when changing a function, especially when it gives data to other functions.
% \item Forces compartmentalisation and documentation of functions.
% \item Really effective in Jupyter Notebooks to test a cell.

% \end{itemize}

% \column{0.5\textwidth}
% \begin{figure}
%     % \centering
%     \includegraphics[height=.7\textheight]{figures/screenshotDoctest.png}
%     \caption{From docs.python.org/3.7/library/doctest.html}
%     \label{fig:documentation}
% \end{figure}

% \end{columns}
% \end{frame}
% \begin{frame}{Doctesting in Jupyter}
% \begin{figure}
%     % \centering
%     \includegraphics[height=.75\textheight]{figures/jupyterDoctest.png}
%     \caption{From code I wrote to help a Masters student analyise their social media data. GPLv3}
%     \label{fig:analysis}
% \end{figure}
% \end{frame}
% \section{Shell Scripts and Jupyter Notebooks}
% \begin{frame}{Different tools for different purposes}
% \begin{itemize}[label=\textbullet]
%     \item I use Jupyter Notebooks when I'm likely to be sharing the code with researchers.
%     \item See the LIGO black hole mybinder notebook.
%     \item Notebooks are fundamentally narrative and chunked. Tell a story. Encourage Documentation.
%     \item Shell scripts (and python run from the shell) is a "Just run everything." Scales better. Less nonsense, more capable. Run and forget.
% \end{itemize}

% \end{frame}


% \begin{frame}{Thank you!}

% % This presentation is available at:
% % \texttt{https://osf.io/...}

% Source code for this presentation is available at: \url{https://github.com/Denubis/Lessons-learned-from-Data-Analysis-projects}

% This work is licensed under a Creative Commons Attribution 4.0 International License.

% \end{frame}



\end{document}